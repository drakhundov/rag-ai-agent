{
    "MODELS": {
        "DEFAULT_CHAT_MODEL": "openai/gpt-oss-20b",
        "DEFAULT_EMB_MODEL": "BAAI/bge-m3"
    },
    "CACHE_DIR": "${PROJ_DIR}/cache",
    "CHROMA_INDEX_DIR": "${CACHE_DIR}/chroma_index",
    "LOGS_DIR": "${PROJ_DIR}/logs",
    "SESSIONS_DIR": "${PROJ_DIR}/sessions",
    "ROUTER_SESSIONS_DIR": "${SESSIONS_DIR}/router",
    "HF_ROUTER_URL": "https://router.huggingface.co/v1",
    "LANGSMITH_API_URL": "https://api.smith.langchain.com",
    "PROMPT_TEMPLATES": {
        "SYSTEM": {
            "INPUT_VARIABLES": [
                "context",
                "query"
            ],
            "TEMPLATE": "You are a careful, literal customer assistant. Answer strictly and only from the provided context. If the answer is not fully supported by the context, say: \"I don't know based on the provided document.\" Be concise and factual.\n\nContext:\n{context}\n\nQuestion:\n{query}\n\nRules:\n1) Do not use outside knowledge.\n2) If multiple pages disagree, state what each one claims.\n3) If the context is insufficient, say so.\n4) Whenever possible, quote numbers/names exactly as they appear.\n5) Provide page references if possible (p.X).\n\nAnswer:"
        },
        "MULTI_QUERY_RAG_PROMPT": {
            "INPUT_VARIABLES": [
                "query",
                "quantity"
            ],
            "TEMPLATE": "You are an AI language model assistant. Your task is to generate {quantity} different versions of the given user query to retrieve relevant documents from a vector database. By generating multiple perspectives on the user query, your goal is to help the user overcome some of the limitations of the distance-based similarity search. Provide these alternative querys separated by newlines. Original query: {query}"
        },
        "HYDE_RAG_PROMPT": {
            "INPUT_VARIABLES": [
                "query",
                "max_tokens"
            ],
            "TEMPLATE": "Generate a scientific paper passage to answer the query. Maximum number of tokens: {max_tokens}.\nQuestion: {query}\n\nPassage:"
        },
        "DECOMPOSITION_RAG_PROMPT": {
            "INPUT_VARIABLES": [
                "query",
                "quantity"
            ],
            "TEMPLATE": "Decompose the following question into {quantity} simpler sub-questions that can each be answered with a short passage from a document without numbering and without bold or italic text:\nQuestion: {query}\nSub-questions:"
        },
        "STEP_BACK_RAG_PROMPT": {
            "INPUT_VARIABLES": [
                "query",
                "quantity"
            ],
            "TEMPLATE": "Generalize the following question into {quantity} more abstract questions that can each be answered with a short passage from a document (without numbering and without bold or italic text):\nQuestion: {query}\nAbstract questions:"
        }
    },
    "SENTENCE_CONCAT_BUFSZ": 2,
    "BREAKPOINT_PERCENTILE_THRESHOLD": 95
}